{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b36f46-d6bc-4581-899a-fa8d26fcd390",
   "metadata": {},
   "source": [
    "# Phrases\n",
    "\n",
    "Process the phrases that appear in a relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d0a2ff-aa89-462a-b714-22f7dd45f738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331f1bd-85e7-417d-b5e7-a656ae6c4e96",
   "metadata": {},
   "source": [
    "## 1. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "264cfc81-f029-4b36-a957-c2ca1fd1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"../data/femke.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775c6f98-a64a-4c47-949b-abbf60870e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "infile = open(DATA_FILE, \"r\")\n",
    "for line in infile:\n",
    "    json_data.append(json.loads(line))\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "426684f2-2450-4945-9be1-2cf22f398cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1867"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2eb4bd-67f3-47c6-a524-4630aee9ae4e",
   "metadata": {},
   "source": [
    "## 2. Give an example of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0d2e1f8-82cf-4e04-8c53-f01548c25616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6660,\n",
       " 'data': 'Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'label': [[734, 772, 'Content_Concept_2'],\n",
       "  [731, 733, 'Content_Relation_Explanation'],\n",
       "  [616, 678, 'Content_Concept_1']],\n",
       " 'source_id': 3487,\n",
       " 'speech_id': 536,\n",
       " 'paragraph_id': '1-2',\n",
       " 'missing concept 1': 'you in Greece, with out support, you need to rebuild your country'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2828f6-6246-475b-abb0-a4a2df26caf7",
   "metadata": {},
   "source": [
    "## 3. Count the frequency of the labels in each data item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b6a32b-980b-4ec7-9efe-2a3f88bcf594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1 1 1': 1400, '2 1 1': 230, '1 1 2': 136, '2 1 2': 24, '1 2 1': 2, '1 0 0': 3, '1 1 0': 5, '0 1 1': 5, '3 1 1': 30, '1 1 3': 13, '2 1 0': 3, '4 1 1': 4, '0 1 3': 1, '3 1 2': 3, '2 1 3': 3, '5 1 1': 1, '1 1 4': 1, '4 1 2': 3}\n"
     ]
    }
   ],
   "source": [
    "def get_patterns(json_data):\n",
    "    base_cases = []\n",
    "    results = {}\n",
    "    for data in json_data:\n",
    "        key = [0, 0, 0]\n",
    "        for label in data[\"label\"]:\n",
    "            if label[2] == 'Content_Concept_1':\n",
    "                key[0] += 1\n",
    "            elif label[2] == 'Content_Relation_Explanation':\n",
    "                key[1] += 1\n",
    "            elif label[2] == 'Content_Concept_2':\n",
    "                key[2] += 1\n",
    "            else:\n",
    "                print(\"cannot happen\")\n",
    "        for i in range(0, len(key)):\n",
    "            key[i] = str(key[i])\n",
    "        results_key = \" \".join(key)\n",
    "        if not results_key in results:\n",
    "            results[results_key] = 1\n",
    "        else:\n",
    "            results[results_key] += 1\n",
    "        if results_key == \"1 1 1\":\n",
    "             base_cases.append(data)\n",
    "    return [results, base_cases]\n",
    "\n",
    "results, base_cases = get_patterns(json_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc8213-d58d-4600-ab5f-4ea454ea275d",
   "metadata": {},
   "source": [
    "Some data items have more than two content concepts because of split phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf504fc9-f8bd-4545-81b4-04e57197b647",
   "metadata": {},
   "source": [
    "## 4. Convert base cases (1,1,1) to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "666fbd9d-a75c-4d69-8072-0b5a23565557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49fae2c-8bce-4508-a1b2-946ddfede36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6660,\n",
       " 'data': 'Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'label': [[734, 772, 'Content_Concept_2'],\n",
       "  [731, 733, 'Content_Relation_Explanation'],\n",
       "  [616, 678, 'Content_Concept_1']],\n",
       " 'source_id': 3487,\n",
       " 'speech_id': 536,\n",
       " 'paragraph_id': '1-2',\n",
       " 'missing concept 1': 'you in Greece, with out support, you need to rebuild your country'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_cases[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317d0cc7-d5fd-48fc-8b72-27a3c5ce77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for data_in in base_cases:\n",
    "    data_out = [data_in[\"data\"], \"\", \"\", \"\"]\n",
    "    for label_data in data_in[\"label\"]:\n",
    "        data_out_id = -1\n",
    "        if label_data[2] == 'Content_Concept_1':\n",
    "            data_out_id = 1\n",
    "        elif label_data[2] == 'Content_Relation_Explanation':\n",
    "            data_out_id = 2\n",
    "        elif label_data[2] == 'Content_Concept_2':\n",
    "            data_out_id = 3\n",
    "        else:\n",
    "            print(f\"unexpected label data: {label_data}\")\n",
    "        if data_out[data_out_id] != \"\":\n",
    "            print(\"duplicate data in label_data: {data_in['label']}\")\n",
    "        data_out[data_out_id] = data_in[\"data\"][label_data[0]:label_data[1]]\n",
    "    table.append(data_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27dbd16-6532-420b-b33a-17a7a4385321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'You, in Greece, with our support, need to rebuild your country',\n",
       " 'to',\n",
       " 'increase the competitiveness of Greece']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90435022-4734-43e4-b66f-24f229187589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paragraph</th>\n",
       "      <th>Content_Concept_1</th>\n",
       "      <th>Content_Relation_Explanation</th>\n",
       "      <th>Content_Concept_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today I want to send a clear message to the pe...</td>\n",
       "      <td>You, in Greece, with our support, need to rebu...</td>\n",
       "      <td>to</td>\n",
       "      <td>increase the competitiveness of Greece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Today I want to send a clear message to the pe...</td>\n",
       "      <td>You, in Greece, with our support, need to rebu...</td>\n",
       "      <td>And the best hope of a</td>\n",
       "      <td>return to growth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To conclude, let me say a few words on the eur...</td>\n",
       "      <td>We have taken important, fundamental decisions</td>\n",
       "      <td>to safeguard</td>\n",
       "      <td>the stability of the euro area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To conclude, let me say a few words on the eur...</td>\n",
       "      <td>We need sustained efforts and determination</td>\n",
       "      <td>As we said there will not be</td>\n",
       "      <td>magic solutions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giving to the ECB the ultimate responsibility ...</td>\n",
       "      <td>confidence between the banks</td>\n",
       "      <td>and in this way</td>\n",
       "      <td>increase the financial stability in the euro area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>But today I want to focus on our economic prio...</td>\n",
       "      <td>cut business taxes</td>\n",
       "      <td>You've got to</td>\n",
       "      <td>succeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>But today I want to focus on our economic prio...</td>\n",
       "      <td>tackle the bloat in welfare</td>\n",
       "      <td>You've got to</td>\n",
       "      <td>succeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>But today I want to focus on our economic prio...</td>\n",
       "      <td>make sure your schools and your universities a...</td>\n",
       "      <td>and crucially you've got to</td>\n",
       "      <td>succeed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>Now yesterday I gave a speech setting out the ...</td>\n",
       "      <td>When you have a single currency</td>\n",
       "      <td>you move inexorably towards</td>\n",
       "      <td>a banking union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>Now yesterday I gave a speech setting out the ...</td>\n",
       "      <td>When you have a single currency</td>\n",
       "      <td>you move inexorably towards</td>\n",
       "      <td>forms of fiscal union</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Paragraph  \\\n",
       "0     Today I want to send a clear message to the pe...   \n",
       "1     Today I want to send a clear message to the pe...   \n",
       "2     To conclude, let me say a few words on the eur...   \n",
       "3     To conclude, let me say a few words on the eur...   \n",
       "4     Giving to the ECB the ultimate responsibility ...   \n",
       "...                                                 ...   \n",
       "1395  But today I want to focus on our economic prio...   \n",
       "1396  But today I want to focus on our economic prio...   \n",
       "1397  But today I want to focus on our economic prio...   \n",
       "1398  Now yesterday I gave a speech setting out the ...   \n",
       "1399  Now yesterday I gave a speech setting out the ...   \n",
       "\n",
       "                                      Content_Concept_1  \\\n",
       "0     You, in Greece, with our support, need to rebu...   \n",
       "1     You, in Greece, with our support, need to rebu...   \n",
       "2        We have taken important, fundamental decisions   \n",
       "3           We need sustained efforts and determination   \n",
       "4                          confidence between the banks   \n",
       "...                                                 ...   \n",
       "1395                                 cut business taxes   \n",
       "1396                        tackle the bloat in welfare   \n",
       "1397  make sure your schools and your universities a...   \n",
       "1398                    When you have a single currency   \n",
       "1399                    When you have a single currency   \n",
       "\n",
       "      Content_Relation_Explanation  \\\n",
       "0                               to   \n",
       "1           And the best hope of a   \n",
       "2                     to safeguard   \n",
       "3     As we said there will not be   \n",
       "4                  and in this way   \n",
       "...                            ...   \n",
       "1395                 You've got to   \n",
       "1396                 You've got to   \n",
       "1397   and crucially you've got to   \n",
       "1398   you move inexorably towards   \n",
       "1399   you move inexorably towards   \n",
       "\n",
       "                                      Content_Concept_2  \n",
       "0                increase the competitiveness of Greece  \n",
       "1                                      return to growth  \n",
       "2                        the stability of the euro area  \n",
       "3                                       magic solutions  \n",
       "4     increase the financial stability in the euro area  \n",
       "...                                                 ...  \n",
       "1395                                            succeed  \n",
       "1396                                            succeed  \n",
       "1397                                            succeed  \n",
       "1398                                    a banking union  \n",
       "1399                              forms of fiscal union  \n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(table, columns=[\"Paragraph\", \"Content_Concept_1\", \"Content_Relation_Explanation\", \"Content_Concept_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e5de83-1ea6-4ee7-b72d-0da9a58df4a4",
   "metadata": {},
   "source": [
    "## 5. Combine all data of duplicate paragraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc106bb-6da3-430c-ae11-550527b930d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = {}\n",
    "for data in json_data:\n",
    "    key = f\"{data['source_id']} {data['speech_id']} {data['paragraph_id']}\"\n",
    "    if key not in combined_data:\n",
    "        combined_data[key] = copy.deepcopy(data)\n",
    "    else:\n",
    "        if len(data[\"data\"]) != len(combined_data[key][\"data\"]):\n",
    "            print(\"cannot happen\")\n",
    "        for label_data in data[\"label\"]:\n",
    "            if label_data not in combined_data[key][\"label\"]:\n",
    "                combined_data[key][\"label\"].append(label_data)\n",
    "                if combined_data[key][\"label\"][-1][1] > len(combined_data[key][\"data\"]):\n",
    "                    combined_data[key][\"label\"][-1][1] = len(combined_data[key][\"data\"])\n",
    "\n",
    "for key in combined_data:\n",
    "    for label_data in combined_data[key][\"label\"]:\n",
    "        label_data.append(combined_data[key][\"data\"][label_data[0]:label_data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "098760d4-a38b-4ba8-8150-c87ab4b5dde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca54a8b-7175-4588-a109-b1b9b4affb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5 2 4': 3, '2 2 2': 52, '1 1 1': 95, '3 2 3': 10, '3 2 2': 24, '5 3 3': 3, '5 2 2': 6, '5 4 6': 5, '4 3 3': 5, '1 2 2': 5, '8 2 4': 1, '2 3 3': 1, '8 5 8': 1, '2 1 1': 30, '1 1 2': 22, '3 3 3': 15, '2 1 2': 8, '9 5 6': 1, '3 3 4': 8, '3 1 2': 5, '2 2 6': 4, '4 2 2': 11, '3 1 1': 12, '4 2 3': 8, '8 3 4': 2, '6 4 4': 3, '2 2 4': 5, '4 1 1': 5, '5 1 1': 4, '14 6 7': 1, '13 4 4': 1, '3 4 4': 2, '1 1 3': 7, '6 2 2': 5, '7 5 5': 2, '4 3 4': 7, '7 4 4': 2, '5 1 2': 1, '2 3 4': 2, '6 4 5': 1, '4 4 4': 5, '3 5 8': 1, '1 3 4': 1, '2 2 3': 9, '6 5 5': 2, '8 5 5': 1, '4 2 1': 2, '7 1 2': 1, '1 1 4': 3, '3 2 5': 2, '6 3 4': 2, '8 3 9': 2, '3 3 5': 1, '6 4 6': 4, '4 2 4': 5, '4 4 5': 1, '10 9 11': 1, '15 11 16': 1, '11 5 6': 1, '1 1 5': 3, '10 3 4': 1, '1 1 8': 1, '8 3 11': 1, '1 2 3': 2, '14 5 9': 1, '3 2 4': 2, '7 5 6': 1, '4 5 6': 1, '7 2 2': 2, '9 4 6': 2, '6 1 1': 2, '4 3 2': 1, '9 4 11': 1, '4 1 3': 3, '2 1 12': 1, '3 1 4': 2, '11 4 6': 1, '3 5 7': 1, '6 3 5': 1, '2 1 5': 1, '4 4 7': 1, '2 2 5': 2, '2 3 5': 2, '11 3 4': 1, '1 2 5': 1, '5 4 4': 3, '5 2 10': 1, '11 7 9': 2, '8 5 7': 1, '4 2 5': 2, '2 1 8': 1, '14 8 14': 1, '14 3 8': 1, '9 7 14': 1, '7 6 8': 1, '10 1 1': 1, '6 5 4': 1, '2 3 9': 1, '5 5 4': 1, '9 5 13': 1, '6 2 6': 1, '6 3 3': 2, '4 1 2': 3, '6 1 3': 1, '6 3 2': 1, '2 4 4': 1, '3 1 3': 2, '10 7 8': 1, '5 3 6': 1, '6 2 5': 1, '5 5 5': 1, '7 3 6': 1, '5 2 6': 1, '15 3 8': 1, '2 1 3': 2, '12 4 6': 1, '5 4 7': 1, '1 2 6': 1, '4 4 6': 1, '4 2 6': 1, '5 5 7': 1, '7 6 10': 1, '14 2 3': 1, '7 5 13': 1, '3 3 2': 1, '11 5 11': 1, '5 3 7': 2, '1 3 3': 1, '5 3 4': 1, '4 2 9': 1, '9 3 3': 1, '7 2 5': 1, '6 2 3': 1}\n"
     ]
    }
   ],
   "source": [
    "results, base_cases = get_patterns(list(combined_data.values()))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e8e0e8-9aed-4cd4-95f1-27b677aba6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 6660,\n",
       " 'data': 'Today I want to send a clear message to the people of this great country, of Greece. I know that many people feel without hope. Many are making extremely difficult sacrifices. And many people ask why they should do more. I understand those concerns. And I agree that some of the efforts seem unfair. But I ask people to recognise the other alternatives which will be much more difficult for Greece and will affect even more the most vulnerable in the Greek society. So this is why it is the right approach to ask Greece to reform, to increase its competitiveness to have a viable future, irrespective of the crisis. You, in Greece, with our support, need to rebuild your country, your structures, your administration, your economy to increase the competitiveness of Greece. And the best hope of a return to growth and job creation is inside the euro area. Staying in the euro is the best chance to avoid worse hardship and difficulties to the Greek people, namely for those in a more vulnerable position',\n",
       " 'label': [[734,\n",
       "   772,\n",
       "   'Content_Concept_2',\n",
       "   'increase the competitiveness of Greece'],\n",
       "  [731, 733, 'Content_Relation_Explanation', 'to'],\n",
       "  [616,\n",
       "   678,\n",
       "   'Content_Concept_1',\n",
       "   'You, in Greece, with our support, need to rebuild your country'],\n",
       "  [616,\n",
       "   665,\n",
       "   'Content_Concept_1',\n",
       "   'You, in Greece, with our support, need to rebuild'],\n",
       "  [680, 695, 'Content_Concept_1', 'your structures'],\n",
       "  [697, 716, 'Content_Concept_1', 'your administration'],\n",
       "  [718, 730, 'Content_Concept_1', 'your economy'],\n",
       "  [774, 796, 'Content_Relation_Explanation', 'And the best hope of a'],\n",
       "  [797, 813, 'Content_Concept_2', 'return to growth'],\n",
       "  [797, 806, 'Content_Concept_2', 'return to'],\n",
       "  [818, 830, 'Content_Concept_2', 'job creation']],\n",
       " 'source_id': 3487,\n",
       " 'speech_id': 536,\n",
       " 'paragraph_id': '1-2',\n",
       " 'missing concept 1': 'you in Greece, with out support, you need to rebuild your country'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[list(combined_data.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3ad6c2-7207-4ed8-871c-bc58c6a63fb7",
   "metadata": {},
   "source": [
    "## 6. Make character labels\n",
    "\n",
    "Several tokens have more than one label. We use the following labeling scheme:\n",
    "\n",
    "* 1: Content_Concept_1\n",
    "* 2: Content_Concept_2\n",
    "* 3: both Content_Concept_1 and Content_Concept_2\n",
    "* E: Content_Relation_Explanation\n",
    "* F: both Content_Relation_Explanation and Content_Concept_1\n",
    "* G: both Content_Relation_Explanation and Content_Concept_2\n",
    "* \\*: all three labels: Content_Relation_Explanation and Content_Concept_1 and Content_Concept_2\n",
    "* .: no label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ba21f5-31fb-4a11-88a6-1bc31f517bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Label_Clash:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "        \n",
    "    def add(self, key):\n",
    "        if key not in self.data:\n",
    "            self.data[key] = 1\n",
    "        else:\n",
    "            self.data[key] += 1\n",
    "            \n",
    "    def print(self):\n",
    "        print(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "200a302c-6e67-4e8c-af05-eb55bd33b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_clash = Label_Clash()\n",
    "for key in combined_data:\n",
    "    combined_data[key]['labels'] = len(combined_data[key][\"data\"])*[\".\"]\n",
    "    for label in combined_data[key][\"label\"]:\n",
    "        for i in range(label[0], label[1]):\n",
    "            if label[2] == \"Content_Concept_1\":\n",
    "                if combined_data[key]['labels'][i] != \".\":\n",
    "                    combined_data[key]['labels'][i] = \"1\"\n",
    "                elif combined_data[key]['labels'][i] != \"1\":\n",
    "                    combined_data[key]['labels'][i] = \"1\"\n",
    "                elif combined_data[key]['labels'][i] != \"E\":\n",
    "                    combined_data[key]['labels'][i] = \"F\"\n",
    "                elif combined_data[key]['labels'][i] != \"2\":\n",
    "                    combined_data[key]['labels'][i] = \"3\"\n",
    "                elif combined_data[key]['labels'][i] != \"3\":\n",
    "                    combined_data[key]['labels'][i] = \"3\"\n",
    "                elif combined_data[key]['labels'][i] != \"F\":\n",
    "                    combined_data[key]['labels'][i] = \"F\"\n",
    "                elif combined_data[key]['labels'][i] != \"G\":\n",
    "                    combined_data[key]['labels'][i] = \"*\"\n",
    "                else:\n",
    "                    print(\"cannot happen\")\n",
    "            elif label[2] == \"Content_Relation_Explanation\":\n",
    "                if combined_data[key]['labels'][i] != \".\":\n",
    "                    combined_data[key]['labels'][i] = \"E\"\n",
    "                elif combined_data[key]['labels'][i] != \"1\":\n",
    "                    combined_data[key]['labels'][i] = \"F\"\n",
    "                elif combined_data[key]['labels'][i] != \"E\":\n",
    "                    combined_data[key]['labels'][i] = \"E\"\n",
    "                elif combined_data[key]['labels'][i] != \"2\":\n",
    "                    combined_data[key]['labels'][i] = \"G\"\n",
    "                elif combined_data[key]['labels'][i] != \"3\":\n",
    "                    combined_data[key]['labels'][i] = \"*\"\n",
    "                elif combined_data[key]['labels'][i] != \"F\":\n",
    "                    combined_data[key]['labels'][i] = \"F\"\n",
    "                elif combined_data[key]['labels'][i] != \"G\":\n",
    "                    combined_data[key]['labels'][i] = \"G\"\n",
    "                else:\n",
    "                    print(\"cannot happen\")\n",
    "            elif label[2] == \"Content_Concept_2\":\n",
    "                if combined_data[key]['labels'][i] != \".\":\n",
    "                    combined_data[key]['labels'][i] = \"2\"\n",
    "                elif combined_data[key]['labels'][i] != \"1\":\n",
    "                    combined_data[key]['labels'][i] = \"3\"\n",
    "                elif combined_data[key]['labels'][i] != \"E\":\n",
    "                    combined_data[key]['labels'][i] = \"G\"\n",
    "                elif combined_data[key]['labels'][i] != \"2\":\n",
    "                    combined_data[key]['labels'][i] = \"2\"\n",
    "                elif combined_data[key]['labels'][i] != \"3\":\n",
    "                    combined_data[key]['labels'][i] = \"3\"\n",
    "                elif combined_data[key]['labels'][i] != \"F\":\n",
    "                    combined_data[key]['labels'][i] = \"*\"\n",
    "                elif combined_data[key]['labels'][i] != \"G\":\n",
    "                    combined_data[key]['labels'][i] = \"G\"\n",
    "                else:\n",
    "                    print(\"cannot happen\")\n",
    "            else:\n",
    "                print(f\"unknown label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9608b557-2475-4ca6-9522-e2d93a288e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................11111111111111111111111111111111111111111111111111111111111111..111111111111111..1111111111111111111..111111111111.FF.33333333333333333333333333333333333333..FFFFFFFFFFFFFFFFFFFFFF.2222222223333333.....333333333333.......................................................................................................'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\"\".join(combined_data[list(combined_data.keys())[0]][\"labels\"]))[:933]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c859119-b183-4698-8aed-165502f9300b",
   "metadata": {},
   "source": [
    "## 7. Machine learning\n",
    "\n",
    "Instructions copied from `filterbubble/transformers/test.ipynb` section `5. BERT Fine-Tuning Tutorial with PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc2d8cfd-e764-4f13-849c-767cde9e1c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForTokenClassification, AdamW, BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5e3083f-73f5-49dc-9801-60532c659f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_labels(predictions):\n",
    "    prediction_labels = []\n",
    "    for i in range(len(predictions)):\n",
    "        prediction_labels.extend(np.argmax(predictions[i], axis=1).flatten())\n",
    "    return prediction_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67354984-ad68-408e-961e-529802f6a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9f51fe1-0bf2-4edf-99d0-cd284799ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e149009e-3dfe-4937-b959-992806b673a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_initial_words(sentence, n):\n",
    "    words = sentence.strip().split()\n",
    "    return \" \".join(words[int(n):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68dda181-a1c0-44a6-ac65-e6a3c1fa777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_labels(true_labels, predictions, sentence_sources, label_values):\n",
    "    prediction_labels = get_prediction_labels(predictions)\n",
    "    true_labels_flattened = []\n",
    "    for array in true_labels:\n",
    "         true_labels_flattened.extend(array)\n",
    "    prediction_labels_collapsed = []\n",
    "    true_labels_collapsed = []\n",
    "    for i in range(0, len(sentence_sources)):\n",
    "        if i == 0 or sentence_sources[i] != sentence_sources[i-1]:\n",
    "            prediction_labels_collapsed.append(prediction_labels[i])\n",
    "            true_labels_collapsed.append(true_labels_flattened[i])\n",
    "        elif prediction_labels[i] != label_values['ANDERS']:\n",
    "            prediction_labels_collapsed[-1] = prediction_labels[i]\n",
    "    return [ true_labels_collapsed, prediction_labels_collapsed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a47d555-6943-4f6d-ad0c-ed98ae2afe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input_ids(sentences, file_labels, keep_short_only=False):\n",
    "    input_ids, attention_masks, expanded_labels, sentence_sources = [], [], [], []\n",
    "    max_length = 64\n",
    "    for i in range(0, len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        while len(sentence) > 0:\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                sentence,\n",
    "                                max_length = max_length,\n",
    "                                truncation=True,\n",
    "                                padding='max_length',\n",
    "                                add_special_tokens = True,\n",
    "                                return_attention_mask = True,\n",
    "                                return_tensors = 'pt',\n",
    "                           )\n",
    "            if keep_short_only and encoded_dict['attention_mask'][0][max_length-1] != 0:\n",
    "                break\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "            expanded_labels.append(file_labels[i])\n",
    "            sentence_sources.append(i)\n",
    "            sentence = remove_initial_words(sentence, int(max_length/2))\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(expanded_labels)\n",
    "    return [input_ids, attention_masks, labels, sentence_sources]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f4b276e-439a-46d4-9a95-c8c191034069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(fold, sentences):\n",
    "    validation_start = int(0.1 * fold * len(sentences))\n",
    "    validation_end = int(0.1 * (fold + 1) * len(sentences))\n",
    "    input_ids, attention_masks, labels, sentence_sources_validation = make_input_ids(sentences[validation_start:validation_end], \n",
    "                                                                                     file_labels[validation_start:validation_end], \n",
    "                                                                                     keep_short_only=False)\n",
    "    val_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    if fold == 0:\n",
    "        training_sentences = []\n",
    "        training_file_labels = []\n",
    "    else:\n",
    "        training_sentences = sentences[:validation_start]\n",
    "        training_file_labels = file_labels[:validation_start]\n",
    "    if fold < 9:\n",
    "        training_sentences.extend(sentences[validation_end:])\n",
    "        training_file_labels.extend(file_labels[validation_end:])\n",
    "    input_ids, attention_masks, labels, _ = make_input_ids(training_sentences, training_file_labels, keep_short_only=True)\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return [ train_dataset, val_dataset, sentence_sources_validation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62ccf8c4-c65b-481d-adf9-940ea4fb4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_experiment(fold, sentences):\n",
    "    train_dataset, val_dataset, sentence_sources_validation = make_data(fold, sentences)\n",
    "    print(f\"fold: {fold}; train size: {len(train_dataset)}; validation size: {len(val_dataset)}\")\n",
    "    batch_size = 32\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,\n",
    "                sampler = RandomSampler(train_dataset),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset,\n",
    "                sampler = SequentialSampler(val_dataset),\n",
    "                batch_size = batch_size\n",
    "            )\n",
    "    optimizer = AdamW(model.parameters(),\n",
    "                      lr = 2e-5,\n",
    "                      eps = 1e-8\n",
    "                    )\n",
    "    epochs = 2\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps = 0,\n",
    "                                                num_training_steps = total_steps)\n",
    "    return [ train_dataset, val_dataset, train_dataloader, validation_dataloader, batch_size, epochs, total_steps, optimizer, scheduler, sentence_sources_validation ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dad24be6-e116-43d7-a299-72a5083768a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, device, optimizer, scheduler):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}    Loss: {:.3f}.'.format(step, len(train_dataloader), elapsed, total_train_loss/step))\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].long().to(device)\n",
    "        model.zero_grad()        \n",
    "        model_output = model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        loss = model_output[\"loss\"]\n",
    "        logits = model_output[\"logits\"]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    return avg_train_loss, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b2d8bf75-aaad-4096-8690-503439bd03c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validation_dataloader, device, sentence_sources_validation, label_values):\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    logits_total, label_ids_total = [], []\n",
    "    for batch in validation_dataloader:\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        with torch.no_grad():        \n",
    "            model_output = model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "        loss = model_output[\"loss\"]\n",
    "        logits = model_output[\"logits\"]\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        logits_total.append(logits)\n",
    "        label_ids_total.append(label_ids)\n",
    "    true_labels_collapsed, prediction_labels_collapsed = collapse_labels(label_ids_total, logits_total, sentence_sources_validation, label_values)\n",
    "    print(confusion_matrix(true_labels_collapsed, prediction_labels_collapsed))\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.3f}\".format(avg_val_accuracy))\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    return [ avg_val_accuracy, avg_val_loss, validation_time, true_labels_collapsed, prediction_labels_collapsed ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6df104-d12d-4f7b-b8c6-e9ecb3254e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af5bc4f5-af69-4eb2-a837-e5b7a301535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "sentences = [ 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten' ]\n",
    "file_labels = [ 1, 0, 1, 0, 1, 0, 1, 0, 1, 0 ]\n",
    "label_values = { \"odd\": 1, \"even\": 0 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "14e36416-422c-45c8-8ca6-fbb0ddca545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0; train size: 9; validation size: 1\n",
      "======== Fold  0 ============\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (576) must match the size of tensor b (9) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15783/308732605.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"======== Fold {fold:2d} ============\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mavg_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mavg_val_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels_collapsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_labels_collapsed\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_sources_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15783/2700684175.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, device, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     18\u001b[0m                              \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                              \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                              labels=b_labels)\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/filterbubble/transformers/venv3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/filterbubble/transformers/venv3/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m-> 1389\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m                 )\n\u001b[1;32m   1391\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (576) must match the size of tensor b (9) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "predicted_labels = []\n",
    "for fold in range(0, 1):\n",
    "    model = RobertaForTokenClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels = num_labels)\n",
    "    train_dataset, val_dataset, train_dataloader, validation_dataloader, batch_size, epochs, total_steps, optimizer, scheduler, sentence_sources_validation = \\\n",
    "        make_experiment(fold, sentences)\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    if torch.cuda.is_available():\n",
    "         torch.cuda.manual_seed_all(seed_val)\n",
    "    training_stats = []\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"======== Fold {fold:2d} ============\")\n",
    "    for epoch_i in range(0, epochs):\n",
    "        avg_train_loss, training_time = train_model(model, train_dataloader, device, optimizer, scheduler)\n",
    "        avg_val_accuracy, avg_val_loss, validation_time, true_labels_collapsed, prediction_labels_collapsed = \\\n",
    "            validate_model(model, validation_dataloader, device, sentence_sources_validation, label_values)\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': avg_val_accuracy,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "    true_labels.extend(true_labels_collapsed)\n",
    "    predicted_labels.extend(prediction_labels_collapsed)\n",
    "    print(\"\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58febb36-7093-4162-b408-670f9ae42aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
